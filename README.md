# ğŸ“„ Document Q&A App with LlamaIndex and FastAPI

This is a full-stack application that enables users to upload PDFs and ask questions based on their content. The project uses **LlamaIndex** for document indexing and retrieval, **FastAPI** for the backend, and **React.js** on the frontend.

---

## ğŸš€ Features

- ğŸ“¤ Upload PDFs via the frontend
- ğŸ§  Ask questions about the document contents
- âš¡ Real-time responses from a powerful LLM using [LlamaIndex]
- ğŸ—ƒï¸ Metadata stored in SQLite/PostgreSQL
- ğŸ“ Document storage on local disk or cloud (e.g., S3)

---

## ğŸ—ï¸ Tech Stack

### Backend
- **FastAPI**
- **LlamaIndex**
- **PostgreSQL/SQLite**
- **Uvicorn**

### Frontend
- **React.js** with [Vite](https://vitejs.dev/)
- **TailwindCSS** and [shadcn/ui](https://ui.shadcn.com/)

---
## ğŸ§  How It Works

1. ğŸ“¨ The user uploads a PDF from the frontend.
2. ğŸ“ The backend stores the file on disk.
3. ğŸ” The app then parses the PDF and generates a vector index using **LlamaIndex**.
4. ğŸ’¬ A **chat engine** is created on top of this index, allowing the user to ask questions about the document.

On **startup**, the app:
- Scans the storage directory for existing PDFs
- Automatically parses and re-indexes them
- Makes them available for querying without requiring re-upload

### ğŸ“š Models Used
- ğŸ”¤ **Embedding model**: `text-embedding-small-3` (from Azure OpenAI)
- ğŸ¤– **LLM model**: `gpt-4o` (from Azure OpenAI)

These are integrated via LlamaIndex to provide context-aware document Q&A functionality.

---

## ğŸ§± Architecture Overview

```txt
[Frontend: React]
       |
       v
[FastAPI Backend]
  |      |       |
  |   [Vector Store (LlamaIndex)(In memory]
  |   [PostgreSQL (Doc metadata)]
  |   [Local/Cloud File Storage]
  |
  v
[OpenAI / LLM API]
